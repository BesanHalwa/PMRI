{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cd973a-3772-4de2-bd70-01b20b84231f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 18:42:10.462287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-19 18:42:10.462315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-19 18:42:10.463158: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-19 18:42:10.467678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-19 18:42:11.010840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03707429-afc5-45a8-a335-d1b0d3aa5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "BUFFER_SIZE = 548000\n",
    "BATCH_SIZE = 64\n",
    "# Each image is 256x256 in size\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "OUTPUT_CHANNELS = 6\n",
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8acfaec-42f4-4a62-add3-5cd7c6c248ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 18:42:11.702583: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.702810: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.732510: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.732724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.732910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.733090: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.846269: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.846481: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.846663: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.846837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.847009: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.847197: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.855248: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.855592: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.855780: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.855959: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.856105: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-11-19 18:42:11.856232: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.856391: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 894 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:21:00.0, compute capability: 8.9\n",
      "2024-11-19 18:42:11.856734: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 1\n",
      "2024-11-19 18:42:11.856827: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-11-19 18:42:11.856979: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 20959 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:41:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "def forwardblock(filters, size, apply_batchnorm=True, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=1, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "  if apply_dropout:\n",
    "    result.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "def upsample(filters, size, apply_dropout=False, apply_batchnorm=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.1))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result\n",
    "\n",
    "def Generator():\n",
    "  inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n",
    "\n",
    "  down_stack = [\n",
    "    downsample(64, 4, apply_batchnorm=True),  # (batch_size, 128, 128, 64),\n",
    "    forwardblock(64, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    downsample(128, 4, apply_batchnorm=True),  # (batch_size, 64, 64, 128),\n",
    "    forwardblock(128, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    downsample(256, 4, apply_batchnorm=True),  # (batch_size, 32, 32, 256),\n",
    "    forwardblock(256, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    downsample(512, 4, apply_batchnorm=True),  # (batch_size, 16, 16, 512),\n",
    "    forwardblock(512, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    downsample(1024, 4, apply_batchnorm=True),  # (batch_size, 8, 8, 1024),\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(512, 4, apply_batchnorm=True),  # (batch_size, 16, 16, 1024),\n",
    "    forwardblock(512, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    upsample(256, 4, apply_batchnorm=True),  # (batch_size, 32, 32, 512),\n",
    "    forwardblock(256, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    upsample(128, 4, apply_batchnorm=True),  # (batch_size, 64, 64, 256),\n",
    "    forwardblock(128, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "    upsample(64, 4, apply_batchnorm=True),  # (batch_size, 128, 128, 128),\n",
    "    forwardblock(64, 4, apply_batchnorm=True, apply_dropout=True),\n",
    "  ]\n",
    "\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='softmax')  # (batch_size, 256, 256, 3)\n",
    "\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = tf.keras.layers.Concatenate()([x, skip])\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "\n",
    "def Discriminator():\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n",
    "  tar = tf.keras.layers.Input(shape=[256, 256, 6], name='target_image')\n",
    "\n",
    "  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n",
    "\n",
    "  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n",
    "  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n",
    "  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n",
    "\n",
    "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n",
    "  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n",
    "\n",
    "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inp, tar], outputs=last)\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  # cast to float64\n",
    "  disc_generated_output = tf.cast(disc_generated_output, tf.float32)\n",
    "  gen_output = tf.cast(gen_output, tf.float32)\n",
    "  target = tf.cast(target, tf.float32)\n",
    "    \n",
    "  # Mean absolute error\n",
    "  #gen_output = tf.squeeze(gen_output, axis=-1)\n",
    "  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "\n",
    "  return total_gen_loss, gan_loss, l1_loss\n",
    "\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss\n",
    "\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba0019a0-1ced-4f55-a137-2ba70350f555",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a971c5c9-1a84-4405-b8aa-c1ae394e0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7534f00-a4bd-4179-a783-6f4d29b2cc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.save_weights('./training_checkpoints/enhanced_unet_Generator_model_weights_ckpt47.h5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb6a8be6-df37-4faa-a11c-557d0c170310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/databatches_npy/batch1training_images.npy\n",
      "(9217, 256, 256, 1)\n",
      "(9217, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch1training_images_flipped_around_xaxis.npy\n",
      "(18433, 256, 256, 1)\n",
      "(18433, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch1training_images_flipped_around_yaxis.npy\n",
      "(27649, 256, 256, 1)\n",
      "(27649, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch2training_images.npy\n",
      "(36865, 256, 256, 1)\n",
      "(36865, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch2training_images_flipped_around_xaxis.npy\n",
      "(46081, 256, 256, 1)\n",
      "(46081, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch2training_images_flipped_around_yaxis.npy\n",
      "(55297, 256, 256, 1)\n",
      "(55297, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch3training_images.npy\n",
      "(64513, 256, 256, 1)\n",
      "(64513, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch3training_images_flipped_around_xaxis.npy\n",
      "(73729, 256, 256, 1)\n",
      "(73729, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch3training_images_flipped_around_yaxis.npy\n",
      "(82945, 256, 256, 1)\n",
      "(82945, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch4training_images.npy\n",
      "(90433, 256, 256, 1)\n",
      "(90433, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch4training_images_flipped_around_xaxis.npy\n",
      "(97921, 256, 256, 1)\n",
      "(97921, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch4training_images_flipped_around_yaxis.npy\n",
      "(105409, 256, 256, 1)\n",
      "(105409, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch5training_images.npy\n",
      "(112897, 256, 256, 1)\n",
      "(112897, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch5training_images_flipped_around_xaxis.npy\n",
      "(120385, 256, 256, 1)\n",
      "(120385, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch5training_images_flipped_around_yaxis.npy\n",
      "(127873, 256, 256, 1)\n",
      "(127873, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch6training_images.npy\n",
      "(135361, 256, 256, 1)\n",
      "(135361, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch6training_images_flipped_around_xaxis.npy\n",
      "(142849, 256, 256, 1)\n",
      "(142849, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n",
      "/mnt/databatches_npy/batch6training_images_flipped_around_yaxis.npy\n",
      "(150337, 256, 256, 1)\n",
      "(150337, 256, 256, 6)\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 18:49:22.141091: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 59114913792 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "\n",
    "train_batches = glob.glob('/mnt/databatches_npy/*training_images*')\n",
    "train_batches.sort()\n",
    "\n",
    "train_lables = glob.glob('/mnt/databatches_npy/*training_labels_hot_encoded*')\n",
    "train_lables.sort()\n",
    "\n",
    "\n",
    "test_batches = glob.glob('/mnt/databatches_npy/*test_images*')\n",
    "test_batches.sort()\n",
    "\n",
    "test_lables = glob.glob('/mnt/databatches_npy/*test_labels_hot_encoded*')\n",
    "test_lables.sort()\n",
    "\n",
    "\n",
    "train_data_x = np.zeros((1, 256, 256, 1)).astype(np.int8)\n",
    "train_data_y = np.zeros((1, 256, 256, 6)).astype(np.int8)\n",
    "\n",
    "for train_batch, train_lable in zip(train_batches,train_lables):\n",
    "    print(train_batch)\n",
    "    x_train = np.load(train_batch)\n",
    "    x_train = np.expand_dims(x_train, axis=-1)\n",
    "    x_train = x_train.astype(np.int8)\n",
    "    \n",
    "    y_train = np.load(train_lable).astype(np.int8)\n",
    "    \n",
    "    train_data_x = np.concatenate((train_data_x, x_train), axis=0)\n",
    "    train_data_y = np.concatenate((train_data_y, y_train), axis=0)\n",
    "    print(train_data_x.shape)\n",
    "    print(train_data_y.shape)\n",
    "    print('-------------------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    \n",
    "    #make dataset\n",
    "    train_data_x = tf.data.Dataset.from_tensor_slices(train_data_x) \n",
    "    train_data_y = tf.data.Dataset.from_tensor_slices(train_data_y)\n",
    "    train_dataset = tf.data.Dataset.zip((train_data_x, train_data_y))\n",
    "\n",
    "    train_dataset = train_dataset.shuffle(1000)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "x_test = np.load(test_batches[0])\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "y_test = np.load(test_lables[0])\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    x_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "    y_test = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "    test_dataset = tf.data.Dataset.zip((x_test, y_test))\n",
    "    \n",
    "    test_dataset = test_dataset.shuffle(1000)\n",
    "    test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02db54eb-182c-4351-b3e8-a55c28c6b3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Fine tuning\\n\\n# Define the Dice Loss function for multi-class segmentation\\ndef dice_loss(y_true, y_pred, smooth=1e-6):\\n    y_true = tf.cast(y_true, tf.float32)  # Convert ground truth to float32\\n    y_pred = tf.cast(y_pred, tf.float32)  # Convert predictions to float32\\n    # Flatten the tensors to compute the intersection and union\\n    y_true_f = tf.reshape(y_true, (-1, y_true.shape[-1]))  # Flatten the one-hot encoded labels\\n    y_pred_f = tf.reshape(y_pred, (-1, y_pred.shape[-1]))  # Flatten the predicted values\\n    \\n    # Calculate intersection and union\\n    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\\n    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\\n    \\n    # Calculate the Dice coefficient for each class\\n    dice = (2. * intersection + smooth) / (union + smooth)\\n    \\n    # Calculate the Dice loss (1 - Dice coefficient)\\n    dice_loss = 1 - tf.reduce_mean(dice)\\n    \\n    return dice_loss\\n\\n# Define the combined loss function (Cross-Entropy + Dice Loss)\\ndef combined_loss(y_true, y_pred, alpha=0.5):\\n    y_true = tf.cast(y_true, tf.float32)  # Convert ground truth to float32\\n    y_pred = tf.cast(y_pred, tf.float32)  # Convert predictions to float32\\n    # Cross-Entropy Loss (categorical)\\n    crossentropy_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))\\n    \\n    # Dice Loss\\n    dice = dice_loss(y_true, y_pred)\\n    \\n    # Combined Loss (weighted sum of cross-entropy and dice loss)\\n    total_loss = alpha * crossentropy_loss + (1 - alpha) * dice\\n    \\n    return total_loss\\n\\n# Example usage:\\n# y_true = ground truth mask (shape: (batch_size, 256, 256, 6))\\n# y_pred = predicted segmentation mask (shape: (batch_size, 256, 256, 6))\\n# loss = combined_loss(y_true, y_pred)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Fine tuning\n",
    "\n",
    "# Define the Dice Loss function for multi-class segmentation\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convert ground truth to float32\n",
    "    y_pred = tf.cast(y_pred, tf.float32)  # Convert predictions to float32\n",
    "    # Flatten the tensors to compute the intersection and union\n",
    "    y_true_f = tf.reshape(y_true, (-1, y_true.shape[-1]))  # Flatten the one-hot encoded labels\n",
    "    y_pred_f = tf.reshape(y_pred, (-1, y_pred.shape[-1]))  # Flatten the predicted values\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n",
    "    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n",
    "    \n",
    "    # Calculate the Dice coefficient for each class\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    \n",
    "    # Calculate the Dice loss (1 - Dice coefficient)\n",
    "    dice_loss = 1 - tf.reduce_mean(dice)\n",
    "    \n",
    "    return dice_loss\n",
    "\n",
    "# Define the combined loss function (Cross-Entropy + Dice Loss)\n",
    "def combined_loss(y_true, y_pred, alpha=0.5):\n",
    "    y_true = tf.cast(y_true, tf.float32)  # Convert ground truth to float32\n",
    "    y_pred = tf.cast(y_pred, tf.float32)  # Convert predictions to float32\n",
    "    # Cross-Entropy Loss (categorical)\n",
    "    crossentropy_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))\n",
    "    \n",
    "    # Dice Loss\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "    \n",
    "    # Combined Loss (weighted sum of cross-entropy and dice loss)\n",
    "    total_loss = alpha * crossentropy_loss + (1 - alpha) * dice\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "# Example usage:\n",
    "# y_true = ground truth mask (shape: (batch_size, 256, 256, 6))\n",
    "# y_pred = predicted segmentation mask (shape: (batch_size, 256, 256, 6))\n",
    "# loss = combined_loss(y_true, y_pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "b382742d-9395-4934-8aff-8007921c52d8",
   "metadata": {},
   "source": [
    "# Channel wise loss\n",
    "\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Compute the Dice coefficient for each channel/class separately, and return the average Dice loss.\n",
    "    \n",
    "    y_true: True segmentation masks (one-hot encoded) of shape (batch_size, height, width, num_classes)\n",
    "    y_pred: Predicted segmentation masks (probabilities from softmax) of shape (batch_size, height, width, num_classes)\n",
    "    smooth: Small constant to avoid division by zero.\n",
    "    \"\"\"\n",
    "    # Ensure the ground truth and predictions are of type float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # Reshape y_true and y_pred to shape (batch_size * height * width, num_classes) for pixel-wise operations\n",
    "    y_true_f = tf.reshape(y_true, (-1, y_true.shape[-1]))  # Flatten the labels (batch_size * height * width, num_classes)\n",
    "    y_pred_f = tf.reshape(y_pred, (-1, y_pred.shape[-1]))  # Flatten the predictions similarly\n",
    "\n",
    "    # Calculate the intersection and union for each channel (class)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n",
    "    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n",
    "\n",
    "    # Calculate the Dice coefficient for each class/channel\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    # Calculate the Dice loss for each class/channel: 1 - Dice coefficient\n",
    "    dice_loss = 1 - dice\n",
    "\n",
    "    # Return the average Dice loss across all classes\n",
    "    return tf.reduce_mean(dice_loss)\n",
    "\n",
    "\n",
    "# Define the combined loss function (Cross-Entropy + Dice Loss)\n",
    "def combined_loss(y_true, y_pred, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Combined loss function: Cross-Entropy Loss + Dice Loss.\n",
    "    The loss is computed for each pixel and each channel independently.\n",
    "    \n",
    "    y_true: True segmentation masks (one-hot encoded) of shape (batch_size, height, width, num_classes)\n",
    "    y_pred: Predicted segmentation masks (probabilities from softmax) of shape (batch_size, height, width, num_classes)\n",
    "    alpha: Weighting factor for the cross-entropy and Dice loss components\n",
    "    \"\"\"\n",
    "    # Ensure the ground truth and predictions are of type float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # Cross-Entropy Loss (Categorical Cross-Entropy for multi-class segmentation)\n",
    "    crossentropy_loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y_true, y_pred))\n",
    "\n",
    "    # Dice Loss\n",
    "    dice = dice_loss(y_true, y_pred)\n",
    "\n",
    "    # Combined Loss (weighted sum of cross-entropy and Dice loss)\n",
    "    total_loss = alpha * crossentropy_loss + (1 - alpha) * dice\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4294f9c-3d75-4c88-a6ca-11187a7e6fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_loss_channel_wise(y_true, y_pred, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Compute channel-wise Dice loss for multi-class segmentation.\n",
    "\n",
    "    y_true: True segmentation masks (one-hot encoded) of shape (batch_size, height, width, num_classes)\n",
    "    y_pred: Predicted segmentation masks (probabilities from softmax) of shape (batch_size, height, width, num_classes)\n",
    "    smooth: Small constant to avoid division by zero\n",
    "    \"\"\"\n",
    "    # Ensure the ground truth and predictions are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # Flatten the tensors for pixel-wise operations: (batch_size * height * width, num_classes)\n",
    "    y_true_f = tf.reshape(y_true, (-1, y_true.shape[-1]))  # Flatten the labels\n",
    "    y_pred_f = tf.reshape(y_pred, (-1, y_pred.shape[-1]))  # Flatten the predictions\n",
    "\n",
    "    # Calculate intersection and union for each class/channel (axis=0 means per class)\n",
    "    intersection = tf.reduce_sum(y_true_f * y_pred_f, axis=0)\n",
    "    union = tf.reduce_sum(y_true_f, axis=0) + tf.reduce_sum(y_pred_f, axis=0)\n",
    "\n",
    "    # Compute Dice coefficient for each class (channel)\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "    # Compute Dice loss for each class (1 - Dice coefficient)\n",
    "    dice_loss_per_channel = 1 - dice\n",
    "\n",
    "    # Return the average Dice loss across all classes\n",
    "    return tf.reduce_mean(dice_loss_per_channel)\n",
    "\n",
    "def pixel_wise_binary_crossentropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute pixel-wise binary cross-entropy loss for multi-class segmentation.\n",
    "\n",
    "    y_true: True segmentation masks (one-hot encoded) of shape (batch_size, height, width, num_classes)\n",
    "    y_pred: Predicted segmentation masks (probabilities from softmax) of shape (batch_size, height, width, num_classes)\n",
    "    \"\"\"\n",
    "    # Ensure the ground truth and predictions are float32\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    # Calculate binary cross-entropy loss for each pixel and each class\n",
    "    bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "    \n",
    "    # Compute binary cross-entropy loss for each class (pixel-wise)\n",
    "    loss_per_channel = bce_loss(y_true, y_pred)\n",
    "\n",
    "    # Return the average loss across all classes and pixels\n",
    "    return tf.reduce_mean(loss_per_channel)\n",
    "\n",
    "\n",
    "def combined_loss(y_true, y_pred, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Combined loss function: Binary Cross-Entropy Loss + Channel-wise Dice Loss.\n",
    "\n",
    "    y_true: True segmentation masks (one-hot encoded) of shape (batch_size, height, width, num_classes)\n",
    "    y_pred: Predicted segmentation masks (probabilities from softmax) of shape (batch_size, height, width, num_classes)\n",
    "    alpha: Weighting factor for the binary cross-entropy and Dice loss components\n",
    "    \"\"\"\n",
    "    # Compute Pixel-wise Binary Cross-Entropy Loss\n",
    "    bce_loss = pixel_wise_binary_crossentropy_loss(y_true, y_pred)\n",
    "\n",
    "    # Compute Channel-wise Dice Loss\n",
    "    dice_loss = dice_loss_channel_wise(y_true, y_pred)\n",
    "\n",
    "    # Combine both losses (weighted sum)\n",
    "    total_loss = alpha * bce_loss + (1 - alpha) * dice_loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6bbdb455-ed65-47bf-8827-463f0c9a05a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Define your optimizer (Adam in this case)\\noptimizer = tf.keras.optimizers.Adam()\\n\\n# Function to train on one batch\\n@tf.function\\ndef train_step(x_batch, y_batch):\\n    with tf.GradientTape() as tape:\\n        # Forward pass\\n        y_pred = generator(x_batch, training=True)\\n        \\n        # Calculate the loss\\n        loss = combined_loss(y_batch, y_pred)\\n    \\n    # Compute gradients\\n    grads = tape.gradient(loss, generator.trainable_variables)\\n    \\n    # Update weights\\n    optimizer.apply_gradients(zip(grads, generator.trainable_variables))\\n    \\n    return loss\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Define your optimizer (Adam in this case)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Function to train on one batch\n",
    "@tf.function\n",
    "def train_step(x_batch, y_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        y_pred = generator(x_batch, training=True)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = combined_loss(y_batch, y_pred)\n",
    "    \n",
    "    # Compute gradients\n",
    "    grads = tape.gradient(loss, generator.trainable_variables)\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e59cb4b0-a9d5-4dbb-87c5-af7c89c2dad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# training \\ndef train(model, train_dataset, val_dataset, epochs, batch_size, weights_path=None, save_interval=1):\\n    # Load pre-trained weights if provided\\n    if weights_path:\\n        model.load_weights(weights_path)\\n        print(f\"Loaded weights from {weights_path}\")\\n\\n    for epoch in range(epochs):\\n        print(f\"Epoch {epoch + 1}/{epochs}\")\\n        \\n        # Training phase\\n        train_loss = 0\\n        for step, (x_batch, y_batch) in enumerate(train_dataset.batch(batch_size)):\\n            loss = train_step(x_batch, y_batch)  # Assuming `train_step` function is defined\\n            train_loss += loss\\n\\n        # Compute average training loss for this epoch\\n        train_loss /= step + 1\\n        print(f\"Training Loss: {train_loss.numpy()}\")\\n\\n        # Validation phase\\n        val_loss = 0\\n        for x_batch_val, y_batch_val in val_dataset.batch(batch_size):\\n            y_pred_val = model(x_batch_val, training=False)\\n            val_loss += combined_loss(y_batch_val, y_pred_val)  # Assuming `combined_loss` function is defined\\n\\n        # Compute average validation loss\\n        val_loss /= len(val_dataset)\\n        print(f\"Validation Loss: {val_loss.numpy()}\")\\n\\n        # Save model weights periodically (every `save_interval` epochs)\\n        if (epoch + 1) % save_interval == 0:\\n            save_path = f\\'checkpoint_epoch_{epoch + 1}.h5\\'\\n            model.save_weights(save_path)\\n            print(f\"Model weights saved to {save_path}\")\\n\\n# Example usage:\\n# Assuming `train_dataset` and `val_dataset` are TensorFlow Dataset objects\\n# containing image-mask pairs and you\\'ve defined `train_step` and `combined_loss`\\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# training \n",
    "def train(model, train_dataset, val_dataset, epochs, batch_size, weights_path=None, save_interval=1):\n",
    "    # Load pre-trained weights if provided\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "        print(f\"Loaded weights from {weights_path}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_loss = 0\n",
    "        for step, (x_batch, y_batch) in enumerate(train_dataset.batch(batch_size)):\n",
    "            loss = train_step(x_batch, y_batch)  # Assuming `train_step` function is defined\n",
    "            train_loss += loss\n",
    "\n",
    "        # Compute average training loss for this epoch\n",
    "        train_loss /= step + 1\n",
    "        print(f\"Training Loss: {train_loss.numpy()}\")\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = 0\n",
    "        for x_batch_val, y_batch_val in val_dataset.batch(batch_size):\n",
    "            y_pred_val = model(x_batch_val, training=False)\n",
    "            val_loss += combined_loss(y_batch_val, y_pred_val)  # Assuming `combined_loss` function is defined\n",
    "\n",
    "        # Compute average validation loss\n",
    "        val_loss /= len(val_dataset)\n",
    "        print(f\"Validation Loss: {val_loss.numpy()}\")\n",
    "\n",
    "        # Save model weights periodically (every `save_interval` epochs)\n",
    "        if (epoch + 1) % save_interval == 0:\n",
    "            save_path = f'checkpoint_epoch_{epoch + 1}.h5'\n",
    "            model.save_weights(save_path)\n",
    "            print(f\"Model weights saved to {save_path}\")\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `train_dataset` and `val_dataset` are TensorFlow Dataset objects\n",
    "# containing image-mask pairs and you've defined `train_step` and `combined_loss`\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46aeea48-5bb7-4366-b40d-b75c380d961b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6409f7ab-5946-48d5-bd5a-e60571325595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load initial weights if available, or train from scratch if not\n",
    "#weights_path = './training_checkpoints/enhanced_unet_Generator_model_weights_ckpt47.h5'  \n",
    "#train(generator, train_dataset, test_dataset, epochs=50, batch_size=16, weights_path=weights_path, save_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "404cc8f6-bcbd-4151-82f3-2c46ac43c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(y_true, y_pred):\n",
    "    smooth = 1e-6\n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=(1, 2, 3))  # Element-wise multiplication\n",
    "    union = tf.reduce_sum(y_true, axis=(1, 2, 3)) + tf.reduce_sum(y_pred, axis=(1, 2, 3))\n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return tf.reduce_mean(dice)  # Return the average Dice coefficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a9222-cc9d-4dc8-8eaf-75414e79c33d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9846a7cf-f9cb-4f62-a3ed-34cdcaf87753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5059c580-5df5-4227-afb0-229fdbebac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from ./training_checkpoints/enhanced_unet_Generator_model_weights_ckpt47.h5\n"
     ]
    }
   ],
   "source": [
    "weights_path = './training_checkpoints/enhanced_unet_Generator_model_weights_ckpt47.h5' \n",
    "\n",
    "#weights_path = '/mnt/Final_PMRI_repo/lower_limb_calf_muscle_segmentation/training_checkpoints/checkpoint_epoch_35.h5'\n",
    "\n",
    "generator.load_weights(weights_path)\n",
    "print(f\"Loaded weights from {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "484cc9a4-3543-47e7-9bfd-93f6c53c5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(optimizer='adam', loss=combined_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1167b7d0-e7fe-4897-9661-934d254f0518",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 13:16:52.652865: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 59114913792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 13:17:17.645716: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inmodel/sequential_1/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
      "2024-11-19 13:17:17.971320: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2024-11-19 13:17:18.042314: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-19 13:17:18.587934: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-19 13:17:23.608760: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fc561f2c650 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-19 13:17:23.608786: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-11-19 13:17:23.608791: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-11-19 13:17:23.612116: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731993443.680902  105349 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350/2350 [==============================] - ETA: 0s - loss: 0.2453 - dice_coefficient: 0.8532\n",
      "Epoch 1: saving model to checkpoint_epoch_01.h5\n",
      "2350/2350 [==============================] - 558s 229ms/step - loss: 0.2453 - dice_coefficient: 0.8532 - val_loss: 0.3671 - val_dice_coefficient: 0.8623\n",
      "Epoch 2/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.2253 - dice_coefficient: 0.8601\n",
      "Epoch 2: saving model to checkpoint_epoch_02.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.2253 - dice_coefficient: 0.8601 - val_loss: 0.3406 - val_dice_coefficient: 0.8615\n",
      "Epoch 3/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.2211 - dice_coefficient: 0.8613\n",
      "Epoch 3: saving model to checkpoint_epoch_03.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.2211 - dice_coefficient: 0.8613 - val_loss: 0.3231 - val_dice_coefficient: 0.8664\n",
      "Epoch 4/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.2082 - dice_coefficient: 0.8646\n",
      "Epoch 4: saving model to checkpoint_epoch_04.h5\n",
      "2350/2350 [==============================] - 528s 225ms/step - loss: 0.2082 - dice_coefficient: 0.8646 - val_loss: 0.3300 - val_dice_coefficient: 0.8674\n",
      "Epoch 5/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1973 - dice_coefficient: 0.8674\n",
      "Epoch 5: saving model to checkpoint_epoch_05.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1973 - dice_coefficient: 0.8675 - val_loss: 0.3412 - val_dice_coefficient: 0.8682\n",
      "Epoch 6/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1857 - dice_coefficient: 0.8708\n",
      "Epoch 6: saving model to checkpoint_epoch_06.h5\n",
      "2350/2350 [==============================] - 525s 224ms/step - loss: 0.1857 - dice_coefficient: 0.8708 - val_loss: 0.3242 - val_dice_coefficient: 0.8691\n",
      "Epoch 7/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1730 - dice_coefficient: 0.8742\n",
      "Epoch 7: saving model to checkpoint_epoch_07.h5\n",
      "2350/2350 [==============================] - 527s 224ms/step - loss: 0.1730 - dice_coefficient: 0.8742 - val_loss: 0.3359 - val_dice_coefficient: 0.8685\n",
      "Epoch 8/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1610 - dice_coefficient: 0.8773\n",
      "Epoch 8: saving model to checkpoint_epoch_08.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1610 - dice_coefficient: 0.8774 - val_loss: 0.3269 - val_dice_coefficient: 0.8694\n",
      "Epoch 9/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1497 - dice_coefficient: 0.8802\n",
      "Epoch 9: saving model to checkpoint_epoch_09.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1497 - dice_coefficient: 0.8802 - val_loss: 0.3327 - val_dice_coefficient: 0.8685\n",
      "Epoch 10/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1395 - dice_coefficient: 0.8829\n",
      "Epoch 10: saving model to checkpoint_epoch_10.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1395 - dice_coefficient: 0.8829 - val_loss: 0.3329 - val_dice_coefficient: 0.8685\n",
      "Epoch 11/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1309 - dice_coefficient: 0.8850\n",
      "Epoch 11: saving model to checkpoint_epoch_11.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1309 - dice_coefficient: 0.8850 - val_loss: 0.3237 - val_dice_coefficient: 0.8702\n",
      "Epoch 12/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1228 - dice_coefficient: 0.8869\n",
      "Epoch 12: saving model to checkpoint_epoch_12.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1228 - dice_coefficient: 0.8870 - val_loss: 0.3290 - val_dice_coefficient: 0.8692\n",
      "Epoch 13/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1166 - dice_coefficient: 0.8884\n",
      "Epoch 13: saving model to checkpoint_epoch_13.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1166 - dice_coefficient: 0.8884 - val_loss: 0.3344 - val_dice_coefficient: 0.8687\n",
      "Epoch 14/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1109 - dice_coefficient: 0.8898\n",
      "Epoch 14: saving model to checkpoint_epoch_14.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1109 - dice_coefficient: 0.8898 - val_loss: 0.3248 - val_dice_coefficient: 0.8691\n",
      "Epoch 15/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1053 - dice_coefficient: 0.8910\n",
      "Epoch 15: saving model to checkpoint_epoch_15.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1053 - dice_coefficient: 0.8911 - val_loss: 0.3160 - val_dice_coefficient: 0.8697\n",
      "Epoch 16/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.1014 - dice_coefficient: 0.8919\n",
      "Epoch 16: saving model to checkpoint_epoch_16.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.1014 - dice_coefficient: 0.8919 - val_loss: 0.3230 - val_dice_coefficient: 0.8695\n",
      "Epoch 17/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0971 - dice_coefficient: 0.8929\n",
      "Epoch 17: saving model to checkpoint_epoch_17.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0971 - dice_coefficient: 0.8929 - val_loss: 0.3207 - val_dice_coefficient: 0.8693\n",
      "Epoch 18/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0936 - dice_coefficient: 0.8936\n",
      "Epoch 18: saving model to checkpoint_epoch_18.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0936 - dice_coefficient: 0.8936 - val_loss: 0.3181 - val_dice_coefficient: 0.8703\n",
      "Epoch 19/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0907 - dice_coefficient: 0.8943\n",
      "Epoch 19: saving model to checkpoint_epoch_19.h5\n",
      "2350/2350 [==============================] - 530s 225ms/step - loss: 0.0907 - dice_coefficient: 0.8943 - val_loss: 0.3188 - val_dice_coefficient: 0.8702\n",
      "Epoch 20/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0884 - dice_coefficient: 0.8947\n",
      "Epoch 20: saving model to checkpoint_epoch_20.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0884 - dice_coefficient: 0.8947 - val_loss: 0.3181 - val_dice_coefficient: 0.8700\n",
      "Epoch 21/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0860 - dice_coefficient: 0.8952\n",
      "Epoch 21: saving model to checkpoint_epoch_21.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0860 - dice_coefficient: 0.8952 - val_loss: 0.3230 - val_dice_coefficient: 0.8700\n",
      "Epoch 22/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0836 - dice_coefficient: 0.8957\n",
      "Epoch 22: saving model to checkpoint_epoch_22.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0836 - dice_coefficient: 0.8957 - val_loss: 0.3176 - val_dice_coefficient: 0.8703\n",
      "Epoch 23/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0812 - dice_coefficient: 0.8962\n",
      "Epoch 23: saving model to checkpoint_epoch_23.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0812 - dice_coefficient: 0.8962 - val_loss: 0.3184 - val_dice_coefficient: 0.8695\n",
      "Epoch 24/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0796 - dice_coefficient: 0.8965\n",
      "Epoch 24: saving model to checkpoint_epoch_24.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0796 - dice_coefficient: 0.8965 - val_loss: 0.3249 - val_dice_coefficient: 0.8697\n",
      "Epoch 25/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0773 - dice_coefficient: 0.8969\n",
      "Epoch 25: saving model to checkpoint_epoch_25.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0773 - dice_coefficient: 0.8969 - val_loss: 0.3228 - val_dice_coefficient: 0.8703\n",
      "Epoch 26/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0758 - dice_coefficient: 0.8972\n",
      "Epoch 26: saving model to checkpoint_epoch_26.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0758 - dice_coefficient: 0.8972 - val_loss: 0.3202 - val_dice_coefficient: 0.8701\n",
      "Epoch 27/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0748 - dice_coefficient: 0.8974\n",
      "Epoch 27: saving model to checkpoint_epoch_27.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0748 - dice_coefficient: 0.8974 - val_loss: 0.3218 - val_dice_coefficient: 0.8701\n",
      "Epoch 28/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0729 - dice_coefficient: 0.8977\n",
      "Epoch 28: saving model to checkpoint_epoch_28.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0729 - dice_coefficient: 0.8978 - val_loss: 0.3214 - val_dice_coefficient: 0.8699\n",
      "Epoch 29/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0721 - dice_coefficient: 0.8979\n",
      "Epoch 29: saving model to checkpoint_epoch_29.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0721 - dice_coefficient: 0.8979 - val_loss: 0.3170 - val_dice_coefficient: 0.8707\n",
      "Epoch 30/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0709 - dice_coefficient: 0.8981\n",
      "Epoch 30: saving model to checkpoint_epoch_30.h5\n",
      "2350/2350 [==============================] - 528s 224ms/step - loss: 0.0709 - dice_coefficient: 0.8981 - val_loss: 0.3137 - val_dice_coefficient: 0.8709\n",
      "Epoch 31/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0700 - dice_coefficient: 0.8982\n",
      "Epoch 31: saving model to checkpoint_epoch_31.h5\n",
      "2350/2350 [==============================] - 528s 225ms/step - loss: 0.0700 - dice_coefficient: 0.8983 - val_loss: 0.3175 - val_dice_coefficient: 0.8709\n",
      "Epoch 32/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0684 - dice_coefficient: 0.8985\n",
      "Epoch 32: saving model to checkpoint_epoch_32.h5\n",
      "2350/2350 [==============================] - 530s 225ms/step - loss: 0.0684 - dice_coefficient: 0.8985 - val_loss: 0.3159 - val_dice_coefficient: 0.8713\n",
      "Epoch 33/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0671 - dice_coefficient: 0.8988\n",
      "Epoch 33: saving model to checkpoint_epoch_33.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0671 - dice_coefficient: 0.8987 - val_loss: 0.3211 - val_dice_coefficient: 0.8708\n",
      "Epoch 34/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0668 - dice_coefficient: 0.8988\n",
      "Epoch 34: saving model to checkpoint_epoch_34.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0668 - dice_coefficient: 0.8988 - val_loss: 0.3164 - val_dice_coefficient: 0.8715\n",
      "Epoch 35/50\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0657 - dice_coefficient: 0.8990\n",
      "Epoch 35: saving model to checkpoint_epoch_35.h5\n",
      "2350/2350 [==============================] - 529s 225ms/step - loss: 0.0657 - dice_coefficient: 0.8990 - val_loss: 0.3194 - val_dice_coefficient: 0.8710\n",
      "Epoch 36/50\n",
      " 786/2350 [=========>....................] - ETA: 5:45 - loss: 0.0664 - dice_coefficient: 0.8980"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 12\u001b[0m\n\u001b[1;32m      4\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_epoch_\u001b[39m\u001b[38;5;132;01m{epoch:02d}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# File path to save weights after each epoch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,              \u001b[38;5;66;03m# Save only the weights, not the entire model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,                \u001b[38;5;66;03m# Save weights at every epoch (not just the best model)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m                            \u001b[38;5;66;03m# Display saving status\u001b[39;00m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model with validation data and the checkpoint callback\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your training dataset (TensorFlow Dataset or numpy array)\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Number of epochs to train\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your validation dataset\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size for training\u001b[39;49;00m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include the checkpoint callback\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:868\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    865\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    866\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    867\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 868\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    872\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    873\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    874\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1323\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1319\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1322\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1323\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m     args,\n\u001b[1;32m   1326\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1327\u001b[0m     executing_eagerly)\n\u001b[1;32m   1328\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/context.py:1486\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1486\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1487\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1488\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1489\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1490\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1491\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1492\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1494\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1495\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1496\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1500\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1501\u001b[0m   )\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the checkpoint callback to save weights after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'checkpoint_epoch_{epoch:02d}.h5',  # File path to save weights after each epoch\n",
    "    save_weights_only=True,              # Save only the weights, not the entire model\n",
    "    save_best_only=False,                # Save weights at every epoch (not just the best model)\n",
    "    verbose=1                            # Display saving status\n",
    ")\n",
    "\n",
    "# Train the model with validation data and the checkpoint callback\n",
    "history = generator.fit(\n",
    "    train_dataset,  # Your training dataset (TensorFlow Dataset or numpy array)\n",
    "    epochs=500,      # Number of epochs to train\n",
    "    validation_data=test_dataset,  # Your validation dataset\n",
    "    batch_size=256,  # Batch size for training\n",
    "    callbacks=[checkpoint_callback]  # Include the checkpoint callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ebfba0-67ec-42ea-8664-51ff3a68555c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "488dfd86-ee9f-4194-ab05-014b8bd128f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4  # Example learning rate\n",
    "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b3dbd3-2c64-45a7-a302-f8741831c852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded weights from /mnt/Final_PMRI_repo/lower_limb_calf_muscle_segmentation/training_checkpoints/checkpoint_epoch_35.h5\n"
     ]
    }
   ],
   "source": [
    "weights_path = '/mnt/Final_PMRI_repo/lower_limb_calf_muscle_segmentation/training_checkpoints/checkpoint_epoch_35.h5'\n",
    "\n",
    "generator.load_weights(weights_path)\n",
    "print(f\"Loaded weights from {weights_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18b1d3e7-30b2-4bcb-bd37-cb0bde134c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.compile(optimizer=adam_optimizer, loss=combined_loss, metrics=[dice_coefficient])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "966ab5f2-089c-4c08-ab5f-d7cff559dad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-19 19:00:13.709359: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-11-19 19:00:21.442508: I external/local_xla/xla/service/service.cc:168] XLA service 0x7ff55a85bbe0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-11-19 19:00:21.442533: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-11-19 19:00:21.442538: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (1): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2024-11-19 19:00:21.445814: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1732014021.515121  135369 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2350/2350 [==============================] - ETA: 0s - loss: 0.1047 - dice_coefficient: 0.8904\n",
      "Epoch 1: saving model to checkpoint_epoch_01.h5\n",
      "2350/2350 [==============================] - 1012s 420ms/step - loss: 0.1047 - dice_coefficient: 0.8904 - val_loss: 0.3201 - val_dice_coefficient: 0.8695\n",
      "Epoch 2/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0958 - dice_coefficient: 0.8924\n",
      "Epoch 2: saving model to checkpoint_epoch_02.h5\n",
      "2350/2350 [==============================] - 970s 413ms/step - loss: 0.0958 - dice_coefficient: 0.8924 - val_loss: 0.3137 - val_dice_coefficient: 0.8694\n",
      "Epoch 3/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0889 - dice_coefficient: 0.8940\n",
      "Epoch 3: saving model to checkpoint_epoch_03.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0889 - dice_coefficient: 0.8940 - val_loss: 0.3176 - val_dice_coefficient: 0.8691\n",
      "Epoch 4/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0843 - dice_coefficient: 0.8950\n",
      "Epoch 4: saving model to checkpoint_epoch_04.h5\n",
      "2350/2350 [==============================] - 974s 415ms/step - loss: 0.0843 - dice_coefficient: 0.8950 - val_loss: 0.3191 - val_dice_coefficient: 0.8691\n",
      "Epoch 5/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0818 - dice_coefficient: 0.8955\n",
      "Epoch 5: saving model to checkpoint_epoch_05.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0818 - dice_coefficient: 0.8955 - val_loss: 0.3187 - val_dice_coefficient: 0.8691\n",
      "Epoch 6/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0799 - dice_coefficient: 0.8960\n",
      "Epoch 6: saving model to checkpoint_epoch_06.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0799 - dice_coefficient: 0.8960 - val_loss: 0.3216 - val_dice_coefficient: 0.8690\n",
      "Epoch 7/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0781 - dice_coefficient: 0.8963\n",
      "Epoch 7: saving model to checkpoint_epoch_07.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0781 - dice_coefficient: 0.8963 - val_loss: 0.3234 - val_dice_coefficient: 0.8688\n",
      "Epoch 8/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0765 - dice_coefficient: 0.8967\n",
      "Epoch 8: saving model to checkpoint_epoch_08.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0765 - dice_coefficient: 0.8967 - val_loss: 0.3231 - val_dice_coefficient: 0.8688\n",
      "Epoch 9/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0751 - dice_coefficient: 0.8970\n",
      "Epoch 9: saving model to checkpoint_epoch_09.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0751 - dice_coefficient: 0.8970 - val_loss: 0.3241 - val_dice_coefficient: 0.8689\n",
      "Epoch 10/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0736 - dice_coefficient: 0.8973\n",
      "Epoch 10: saving model to checkpoint_epoch_10.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0736 - dice_coefficient: 0.8973 - val_loss: 0.3211 - val_dice_coefficient: 0.8691\n",
      "Epoch 11/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0726 - dice_coefficient: 0.8975\n",
      "Epoch 11: saving model to checkpoint_epoch_11.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0726 - dice_coefficient: 0.8975 - val_loss: 0.3196 - val_dice_coefficient: 0.8696\n",
      "Epoch 12/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0720 - dice_coefficient: 0.8977\n",
      "Epoch 12: saving model to checkpoint_epoch_12.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0720 - dice_coefficient: 0.8977 - val_loss: 0.3180 - val_dice_coefficient: 0.8700\n",
      "Epoch 13/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0711 - dice_coefficient: 0.8978\n",
      "Epoch 13: saving model to checkpoint_epoch_13.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0711 - dice_coefficient: 0.8978 - val_loss: 0.3142 - val_dice_coefficient: 0.8704\n",
      "Epoch 14/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0701 - dice_coefficient: 0.8981\n",
      "Epoch 14: saving model to checkpoint_epoch_14.h5\n",
      "2350/2350 [==============================] - 973s 414ms/step - loss: 0.0701 - dice_coefficient: 0.8981 - val_loss: 0.3154 - val_dice_coefficient: 0.8702\n",
      "Epoch 15/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0692 - dice_coefficient: 0.8982\n",
      "Epoch 15: saving model to checkpoint_epoch_15.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0692 - dice_coefficient: 0.8983 - val_loss: 0.3132 - val_dice_coefficient: 0.8705\n",
      "Epoch 16/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0686 - dice_coefficient: 0.8984\n",
      "Epoch 16: saving model to checkpoint_epoch_16.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0686 - dice_coefficient: 0.8983 - val_loss: 0.3124 - val_dice_coefficient: 0.8706\n",
      "Epoch 17/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0678 - dice_coefficient: 0.8985\n",
      "Epoch 17: saving model to checkpoint_epoch_17.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0678 - dice_coefficient: 0.8985 - val_loss: 0.3105 - val_dice_coefficient: 0.8708\n",
      "Epoch 18/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0673 - dice_coefficient: 0.8986\n",
      "Epoch 18: saving model to checkpoint_epoch_18.h5\n",
      "2350/2350 [==============================] - 970s 413ms/step - loss: 0.0673 - dice_coefficient: 0.8986 - val_loss: 0.3124 - val_dice_coefficient: 0.8707\n",
      "Epoch 19/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0669 - dice_coefficient: 0.8987\n",
      "Epoch 19: saving model to checkpoint_epoch_19.h5\n",
      "2350/2350 [==============================] - 970s 413ms/step - loss: 0.0669 - dice_coefficient: 0.8987 - val_loss: 0.3098 - val_dice_coefficient: 0.8709\n",
      "Epoch 20/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0666 - dice_coefficient: 0.8988\n",
      "Epoch 20: saving model to checkpoint_epoch_20.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0666 - dice_coefficient: 0.8987 - val_loss: 0.3081 - val_dice_coefficient: 0.8712\n",
      "Epoch 21/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0662 - dice_coefficient: 0.8988\n",
      "Epoch 21: saving model to checkpoint_epoch_21.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0662 - dice_coefficient: 0.8988 - val_loss: 0.3100 - val_dice_coefficient: 0.8709\n",
      "Epoch 22/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0659 - dice_coefficient: 0.8989\n",
      "Epoch 22: saving model to checkpoint_epoch_22.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0659 - dice_coefficient: 0.8989 - val_loss: 0.3099 - val_dice_coefficient: 0.8710\n",
      "Epoch 23/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0653 - dice_coefficient: 0.8990\n",
      "Epoch 23: saving model to checkpoint_epoch_23.h5\n",
      "2350/2350 [==============================] - 969s 413ms/step - loss: 0.0653 - dice_coefficient: 0.8990 - val_loss: 0.3126 - val_dice_coefficient: 0.8709\n",
      "Epoch 24/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0648 - dice_coefficient: 0.8991\n",
      "Epoch 24: saving model to checkpoint_epoch_24.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0648 - dice_coefficient: 0.8991 - val_loss: 0.3144 - val_dice_coefficient: 0.8707\n",
      "Epoch 25/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0644 - dice_coefficient: 0.8992\n",
      "Epoch 25: saving model to checkpoint_epoch_25.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0644 - dice_coefficient: 0.8992 - val_loss: 0.3142 - val_dice_coefficient: 0.8708\n",
      "Epoch 26/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0641 - dice_coefficient: 0.8992\n",
      "Epoch 26: saving model to checkpoint_epoch_26.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0641 - dice_coefficient: 0.8992 - val_loss: 0.3116 - val_dice_coefficient: 0.8709\n",
      "Epoch 27/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0638 - dice_coefficient: 0.8993\n",
      "Epoch 27: saving model to checkpoint_epoch_27.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0638 - dice_coefficient: 0.8993 - val_loss: 0.3120 - val_dice_coefficient: 0.8708\n",
      "Epoch 28/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0634 - dice_coefficient: 0.8993\n",
      "Epoch 28: saving model to checkpoint_epoch_28.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0634 - dice_coefficient: 0.8993 - val_loss: 0.3109 - val_dice_coefficient: 0.8710\n",
      "Epoch 29/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0629 - dice_coefficient: 0.8994\n",
      "Epoch 29: saving model to checkpoint_epoch_29.h5\n",
      "2350/2350 [==============================] - 974s 415ms/step - loss: 0.0629 - dice_coefficient: 0.8994 - val_loss: 0.3105 - val_dice_coefficient: 0.8710\n",
      "Epoch 30/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0627 - dice_coefficient: 0.8995\n",
      "Epoch 30: saving model to checkpoint_epoch_30.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0627 - dice_coefficient: 0.8995 - val_loss: 0.3108 - val_dice_coefficient: 0.8710\n",
      "Epoch 31/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0624 - dice_coefficient: 0.8995\n",
      "Epoch 31: saving model to checkpoint_epoch_31.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0624 - dice_coefficient: 0.8995 - val_loss: 0.3099 - val_dice_coefficient: 0.8711\n",
      "Epoch 32/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0621 - dice_coefficient: 0.8996\n",
      "Epoch 32: saving model to checkpoint_epoch_32.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0621 - dice_coefficient: 0.8995 - val_loss: 0.3109 - val_dice_coefficient: 0.8712\n",
      "Epoch 33/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0617 - dice_coefficient: 0.8996\n",
      "Epoch 33: saving model to checkpoint_epoch_33.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0617 - dice_coefficient: 0.8996 - val_loss: 0.3113 - val_dice_coefficient: 0.8713\n",
      "Epoch 34/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0613 - dice_coefficient: 0.8997\n",
      "Epoch 34: saving model to checkpoint_epoch_34.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0613 - dice_coefficient: 0.8997 - val_loss: 0.3097 - val_dice_coefficient: 0.8716\n",
      "Epoch 35/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0611 - dice_coefficient: 0.8997\n",
      "Epoch 35: saving model to checkpoint_epoch_35.h5\n",
      "2350/2350 [==============================] - 973s 414ms/step - loss: 0.0611 - dice_coefficient: 0.8997 - val_loss: 0.3085 - val_dice_coefficient: 0.8716\n",
      "Epoch 36/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0608 - dice_coefficient: 0.8998\n",
      "Epoch 36: saving model to checkpoint_epoch_36.h5\n",
      "2350/2350 [==============================] - 972s 414ms/step - loss: 0.0608 - dice_coefficient: 0.8998 - val_loss: 0.3098 - val_dice_coefficient: 0.8714\n",
      "Epoch 37/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0605 - dice_coefficient: 0.8999\n",
      "Epoch 37: saving model to checkpoint_epoch_37.h5\n",
      "2350/2350 [==============================] - 970s 413ms/step - loss: 0.0605 - dice_coefficient: 0.8998 - val_loss: 0.3119 - val_dice_coefficient: 0.8712\n",
      "Epoch 38/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0601 - dice_coefficient: 0.8999\n",
      "Epoch 38: saving model to checkpoint_epoch_38.h5\n",
      "2350/2350 [==============================] - 971s 413ms/step - loss: 0.0601 - dice_coefficient: 0.8999 - val_loss: 0.3106 - val_dice_coefficient: 0.8713\n",
      "Epoch 39/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0601 - dice_coefficient: 0.8999\n",
      "Epoch 39: saving model to checkpoint_epoch_39.h5\n",
      "2350/2350 [==============================] - 970s 413ms/step - loss: 0.0601 - dice_coefficient: 0.8999 - val_loss: 0.3106 - val_dice_coefficient: 0.8714\n",
      "Epoch 40/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0599 - dice_coefficient: 0.8999\n",
      "Epoch 40: saving model to checkpoint_epoch_40.h5\n",
      "2350/2350 [==============================] - 975s 415ms/step - loss: 0.0599 - dice_coefficient: 0.8999 - val_loss: 0.3102 - val_dice_coefficient: 0.8713\n",
      "Epoch 41/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0596 - dice_coefficient: 0.9000\n",
      "Epoch 41: saving model to checkpoint_epoch_41.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0596 - dice_coefficient: 0.9000 - val_loss: 0.3110 - val_dice_coefficient: 0.8713\n",
      "Epoch 42/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0594 - dice_coefficient: 0.9000\n",
      "Epoch 42: saving model to checkpoint_epoch_42.h5\n",
      "2350/2350 [==============================] - 973s 414ms/step - loss: 0.0594 - dice_coefficient: 0.9000 - val_loss: 0.3136 - val_dice_coefficient: 0.8711\n",
      "Epoch 43/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0592 - dice_coefficient: 0.9000\n",
      "Epoch 43: saving model to checkpoint_epoch_43.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0592 - dice_coefficient: 0.9000 - val_loss: 0.3129 - val_dice_coefficient: 0.8712\n",
      "Epoch 44/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0591 - dice_coefficient: 0.9001\n",
      "Epoch 44: saving model to checkpoint_epoch_44.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0591 - dice_coefficient: 0.9001 - val_loss: 0.3129 - val_dice_coefficient: 0.8713\n",
      "Epoch 45/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0589 - dice_coefficient: 0.9001\n",
      "Epoch 45: saving model to checkpoint_epoch_45.h5\n",
      "2350/2350 [==============================] - 965s 411ms/step - loss: 0.0589 - dice_coefficient: 0.9001 - val_loss: 0.3142 - val_dice_coefficient: 0.8712\n",
      "Epoch 46/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0586 - dice_coefficient: 0.9002\n",
      "Epoch 46: saving model to checkpoint_epoch_46.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0586 - dice_coefficient: 0.9002 - val_loss: 0.3135 - val_dice_coefficient: 0.8713\n",
      "Epoch 47/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0584 - dice_coefficient: 0.9002\n",
      "Epoch 47: saving model to checkpoint_epoch_47.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0584 - dice_coefficient: 0.9002 - val_loss: 0.3130 - val_dice_coefficient: 0.8713\n",
      "Epoch 48/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0582 - dice_coefficient: 0.9002\n",
      "Epoch 48: saving model to checkpoint_epoch_48.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0582 - dice_coefficient: 0.9002 - val_loss: 0.3148 - val_dice_coefficient: 0.8710\n",
      "Epoch 49/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0581 - dice_coefficient: 0.9002\n",
      "Epoch 49: saving model to checkpoint_epoch_49.h5\n",
      "2350/2350 [==============================] - 969s 412ms/step - loss: 0.0581 - dice_coefficient: 0.9002 - val_loss: 0.3138 - val_dice_coefficient: 0.8712\n",
      "Epoch 50/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0579 - dice_coefficient: 0.9003\n",
      "Epoch 50: saving model to checkpoint_epoch_50.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0579 - dice_coefficient: 0.9003 - val_loss: 0.3132 - val_dice_coefficient: 0.8712\n",
      "Epoch 51/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0578 - dice_coefficient: 0.9003\n",
      "Epoch 51: saving model to checkpoint_epoch_51.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0578 - dice_coefficient: 0.9003 - val_loss: 0.3117 - val_dice_coefficient: 0.8715\n",
      "Epoch 52/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0576 - dice_coefficient: 0.9003\n",
      "Epoch 52: saving model to checkpoint_epoch_52.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0576 - dice_coefficient: 0.9003 - val_loss: 0.3103 - val_dice_coefficient: 0.8717\n",
      "Epoch 53/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0573 - dice_coefficient: 0.9004\n",
      "Epoch 53: saving model to checkpoint_epoch_53.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0573 - dice_coefficient: 0.9004 - val_loss: 0.3090 - val_dice_coefficient: 0.8718\n",
      "Epoch 54/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0571 - dice_coefficient: 0.9004\n",
      "Epoch 54: saving model to checkpoint_epoch_54.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0571 - dice_coefficient: 0.9004 - val_loss: 0.3084 - val_dice_coefficient: 0.8718\n",
      "Epoch 55/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0570 - dice_coefficient: 0.9004\n",
      "Epoch 55: saving model to checkpoint_epoch_55.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0570 - dice_coefficient: 0.9004 - val_loss: 0.3089 - val_dice_coefficient: 0.8717\n",
      "Epoch 56/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0567 - dice_coefficient: 0.9005\n",
      "Epoch 56: saving model to checkpoint_epoch_56.h5\n",
      "2350/2350 [==============================] - 967s 412ms/step - loss: 0.0567 - dice_coefficient: 0.9004 - val_loss: 0.3103 - val_dice_coefficient: 0.8715\n",
      "Epoch 57/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0566 - dice_coefficient: 0.9005\n",
      "Epoch 57: saving model to checkpoint_epoch_57.h5\n",
      "2350/2350 [==============================] - 967s 411ms/step - loss: 0.0566 - dice_coefficient: 0.9005 - val_loss: 0.3105 - val_dice_coefficient: 0.8716\n",
      "Epoch 58/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0563 - dice_coefficient: 0.9005\n",
      "Epoch 58: saving model to checkpoint_epoch_58.h5\n",
      "2350/2350 [==============================] - 966s 411ms/step - loss: 0.0563 - dice_coefficient: 0.9005 - val_loss: 0.3103 - val_dice_coefficient: 0.8716\n",
      "Epoch 59/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0562 - dice_coefficient: 0.9005\n",
      "Epoch 59: saving model to checkpoint_epoch_59.h5\n",
      "2350/2350 [==============================] - 965s 411ms/step - loss: 0.0562 - dice_coefficient: 0.9005 - val_loss: 0.3102 - val_dice_coefficient: 0.8715\n",
      "Epoch 60/500\n",
      "2349/2350 [============================>.] - ETA: 0s - loss: 0.0561 - dice_coefficient: 0.9006\n",
      "Epoch 60: saving model to checkpoint_epoch_60.h5\n",
      "2350/2350 [==============================] - 968s 412ms/step - loss: 0.0561 - dice_coefficient: 0.9005 - val_loss: 0.3101 - val_dice_coefficient: 0.8716\n",
      "Epoch 61/500\n",
      " 962/2350 [===========>..................] - ETA: 9:22 - loss: 0.0560 - dice_coefficient: 0.8997"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m checkpoint_callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcheckpoint_epoch_\u001b[39m\u001b[38;5;132;01m{epoch:02d}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# File path to save weights after each epoch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,              \u001b[38;5;66;03m# Save only the weights, not the entire model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,                \u001b[38;5;66;03m# Save weights at every epoch (not just the best model)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m                            \u001b[38;5;66;03m# Display saving status\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Train the model with validation data and the checkpoint callback\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your training dataset (TensorFlow Dataset or numpy array)\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Number of epochs to train\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your validation dataset\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Batch size for training\u001b[39;49;00m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Include the checkpoint callback\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/engine/training.py:1813\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1811\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1812\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1813\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:631\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1066\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1068\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:394\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/Eshan/env/tfenv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:360\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define the checkpoint callback to save weights after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    'checkpoint_epoch_{epoch:02d}.h5',  # File path to save weights after each epoch\n",
    "    save_weights_only=True,              # Save only the weights, not the entire model\n",
    "    save_best_only=False,                # Save weights at every epoch (not just the best model)\n",
    "    verbose=1                            # Display saving status\n",
    ")\n",
    "\n",
    "# Train the model with validation data and the checkpoint callback\n",
    "history = generator.fit(\n",
    "    train_dataset,  # Your training dataset (TensorFlow Dataset or numpy array)\n",
    "    epochs=500,      # Number of epochs to train\n",
    "    validation_data=test_dataset,  # Your validation dataset\n",
    "    batch_size=32,  # Batch size for training\n",
    "    callbacks=[checkpoint_callback]  # Include the checkpoint callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c756a64-307d-42db-82da-838b52070b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
